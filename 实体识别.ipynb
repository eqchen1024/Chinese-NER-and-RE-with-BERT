{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUqM2XRgmzK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "0d246278-dda9-49c8-91d0-6e737454e2a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33xhLYwLezYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "l2i_dic = { \"O\": 0,u'B-疾病': 1, u'B-药物': 2, u'B-手术': 3, u'B-症状': 4, u'B-预防措施': 5, u'B-观察方法': 6, u'B-诊断技术': 7,\n",
        "           u'B-物理疗法': 8, u'B-护理技术': 9, u'B-病因': 10, u'B-病理': 11, u'B-医疗指标': 12,u'I-疾病': 13, u'I-药物': 14, u'I-手术': 15, u'I-症状': 16, u'I-预防措施': 17, u'I-观察方法': 18, u'I-诊断技术': 19,\n",
        "           u'I-物理疗法': 20, u'I-护理技术': 21, u'I-病因': 22, u'I-病理': 23, u'I-医疗指标': 24,u'S-疾病': 25, u'S-药物': 26, u'S-手术': 27, u'S-症状': 28, u'S-预防措施': 29, u'S-观察方法': 30, u'S-诊断技术': 31,\n",
        "           u'S-物理疗法': 32, u'S-护理技术': 33, u'S-病因': 34, u'S-病理': 35, u'S-医疗指标': 36,u'E-疾病': 37, u'E-药物': 38, u'E-手术': 39, u'E-症状': 40, u'E-预防措施': 41, u'E-观察方法': 42, u'E-诊断技术': 43,\n",
        "           u'E-物理疗法': 44, u'E-护理技术': 45, u'E-病因': 46, u'E-病理': 47, u'E-医疗指标': 48,\n",
        "           \"<pad>\": 49, \"<start>\": 50, \"<eos>\": 51}\n",
        "i2l_dic = { 0:\"O\",1:u'B-疾病',2: u'B-药物',3: u'B-手术',4: u'B-症状',5: u'B-预防措施',6: u'B-观察方法',7: u'B-诊断技术',\n",
        "           8:u'B-物理疗法',9: u'B-护理技术',10: u'B-病因',11: u'B-病理',12: u'B-医疗指标',13:u'I-疾病',14: u'I-药物',15: u'I-手术',16: u'I-症状',17: u'I-预防措施',18: u'I-观察方法',19: u'I-诊断技术',\n",
        "           20:u'I-物理疗法',21: u'I-护理技术',22: u'I-病因',23: u'I-病理',24: u'I-医疗指标',25:u'S-疾病',26: u'S-药物',27: u'S-手术',28: u'S-症状',29: u'S-预防措施',30: u'S-观察方法',31: u'S-诊断技术',\n",
        "           32:u'S-物理疗法',33: u'S-护理技术',34: u'S-病因',35: u'S-病理',36: u'S-医疗指标',37:u'E-疾病',38: u'E-药物',39: u'E-手术',40: u'E-症状',41: u'E-预防措施',42: u'E-观察方法',43: u'E-诊断技术',\n",
        "           44:u'E-物理疗法',45: u'E-护理技术',46: u'E-病因',47: u'E-病理',48: u'E-医疗指标',\n",
        "           49:\"<pad>\",50: \"<start>\",51: \"<eos>\"}\n",
        "train_file = '/content/drive/My Drive/Colab Notebooks/bert-bilstm-crf-master/bert_lstm_crf_pytorch/data/train.txt'\n",
        "dev_file = '/content/drive/My Drive/Colab Notebooks/bert-bilstm-crf-master/bert_lstm_crf_pytorch/data/dev.txt'\n",
        "vocab_file = '/content/drive/My Drive/chinese_roberta_wwm_ext_pytorch.zip_files/vocab.txt'\n",
        "\n",
        "save_model_dir =  '/content/drive/My Drive/Colab Notebooks/bert-bilstm-crf-master/bert_lstm_crf_pytorch/model'\n",
        "\n",
        "max_length = 250\n",
        "batch_size = 1\n",
        "epochs = 3\n",
        "tagset_size = len(l2i_dic)\n",
        "use_cuda = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC5sYx9XfRVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    def __init__(self, text, label, input_id, label_id, input_mask):\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "        self.input_id = input_id\n",
        "        self.label_id = label_id\n",
        "        self.input_mask = input_mask\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = {}\n",
        "    index = 0\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        while True:\n",
        "            token = reader.readline()\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip()\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "def load_file(file_path):\n",
        "    contents = open(file_path, encoding='utf-8').readlines()\n",
        "    text =[]\n",
        "    label = []\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for line in contents:\n",
        "        if line != '\\n':\n",
        "            line = line.strip().split('\\t')\n",
        "            text.append(line[0])\n",
        "            label.append(line[-1])\n",
        "        else:\n",
        "            texts.append(text)\n",
        "            labels.append(label)\n",
        "            text = []\n",
        "            label = []\n",
        "    return texts, labels\n",
        "\n",
        "def load_data(file_path, max_length, label_dic, vocab):\n",
        "    texts, labels = load_file(file_path)\n",
        "    assert len(texts) == len(labels)\n",
        "    result = []\n",
        "    for i in range(len(texts)):\n",
        "        assert len(texts[i]) == len(labels[i])\n",
        "        token = texts[i]\n",
        "        label = labels[i]\n",
        "        if len(token) > max_length-2:\n",
        "            token = token[0:(max_length-2)]\n",
        "            label = label[0:(max_length-2)]\n",
        "        tokens_f =['[CLS]'] + token + ['[SEP]']\n",
        "        label_f = [\"<start>\"] + label + ['<eos>']\n",
        "        input_ids = [int(vocab[i]) if i in vocab else int(vocab['[UNK]']) for i in tokens_f]\n",
        "        label_ids = [label_dic[i] for i in label_f]\n",
        "        mask_bool=1\n",
        "        input_mask = [mask_bool] * len(input_ids)\n",
        "        while len(input_ids) < max_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            label_ids.append(label_dic['<pad>'])\n",
        "        assert len(input_ids) == max_length\n",
        "        assert len(input_mask) == max_length\n",
        "        assert len(label_ids) == max_length\n",
        "        feature = InputFeatures(text=tokens_f, label=label_f, input_id=input_ids, input_mask=input_mask,\n",
        "                                label_id=label_ids)\n",
        "        result.append(feature)\n",
        "    return result\n",
        "\n",
        "def recover_label(pred_var, gold_var, l2i_dic, i2l_dic):\n",
        "    assert len(pred_var) == len(gold_var)\n",
        "    pred_variable = []\n",
        "    gold_variable = []\n",
        "    for i in range(len(gold_var)):\n",
        "     \n",
        "        start_index = gold_var[i].index(l2i_dic['<start>'])\n",
        "        end_index = gold_var[i].index(l2i_dic['<eos>'])\n",
        "        pred_variable.append(pred_var[i][start_index:end_index])\n",
        "        gold_variable.append(gold_var[i][start_index:end_index])\n",
        "\n",
        "    pred_label = []\n",
        "    gold_label = []\n",
        "    for j in range(len(gold_variable)):\n",
        "        pred_label.append([ i2l_dic[t] for t in pred_variable[j] ])\n",
        "        gold_label.append([ i2l_dic[t] for t in gold_variable[j] ])\n",
        "\n",
        "    return pred_label, gold_label\n",
        "\n",
        "def get_ner_fmeasure(golden_lists, predict_lists, label_type=\"BMES\"):\n",
        "    sent_num = len(golden_lists)\n",
        "    golden_full = []\n",
        "    predict_full = []\n",
        "    right_full = []\n",
        "    right_tag = 0\n",
        "    all_tag = 0\n",
        "    for idx in range(0,sent_num):\n",
        "        # word_list = sentence_lists[idx]\n",
        "        golden_list = golden_lists[idx]\n",
        "        predict_list = predict_lists[idx]\n",
        "        for idy in range(len(golden_list)):\n",
        "            if golden_list[idy] == predict_list[idy]:\n",
        "                right_tag += 1\n",
        "        all_tag += len(golden_list)\n",
        "        if label_type == \"BMES\":\n",
        "            gold_matrix = get_ner_BMES(golden_list)\n",
        "            pred_matrix = get_ner_BMES(predict_list)\n",
        "        else:\n",
        "            gold_matrix = get_ner_BIO(golden_list)\n",
        "            pred_matrix = get_ner_BIO(predict_list)\n",
        "        right_ner = list(set(gold_matrix).intersection(set(pred_matrix)))\n",
        "        golden_full += gold_matrix\n",
        "        predict_full += pred_matrix\n",
        "        right_full += right_ner\n",
        "    right_num = len(right_full)\n",
        "    golden_num = len(golden_full)\n",
        "    predict_num = len(predict_full)\n",
        "    if predict_num == 0:\n",
        "        precision = -1\n",
        "    else:\n",
        "        precision =  (right_num+0.0)/predict_num\n",
        "    if golden_num == 0:\n",
        "        recall = -1\n",
        "    else:\n",
        "        recall = (right_num+0.0)/golden_num\n",
        "    if (precision == -1) or (recall == -1) or (precision+recall) <= 0.:\n",
        "        f_measure = -1\n",
        "    else:\n",
        "        f_measure = 2*precision*recall/(precision+recall)\n",
        "    accuracy = (right_tag+0.0)/all_tag\n",
        "    # print \"Accuracy: \", right_tag,\"/\",all_tag,\"=\",accuracy\n",
        "    print (\"gold_num = \", golden_num, \" pred_num = \", predict_num, \" right_num = \", right_num)\n",
        "    return accuracy, precision, recall, f_measure\n",
        "\n",
        "def reverse_style(input_string):\n",
        "    target_position = input_string.index('[')\n",
        "    input_len = len(input_string)\n",
        "    output_string = input_string[target_position:input_len] + input_string[0:target_position]\n",
        "    return output_string\n",
        "\n",
        "def get_ner_BMES(label_list):\n",
        "    # list_len = len(word_list)\n",
        "    # assert(list_len == len(label_list)), \"word list size unmatch with label list\"\n",
        "    list_len = len(label_list)\n",
        "    begin_label = 'B-'\n",
        "    end_label = 'E-'\n",
        "    single_label = 'S-'\n",
        "    whole_tag = ''\n",
        "    index_tag = ''\n",
        "    tag_list = []\n",
        "    stand_matrix = []\n",
        "    for i in range(0, list_len):\n",
        "        # wordlabel = word_list[i]\n",
        "        current_label = label_list[i].upper()\n",
        "        if begin_label in current_label:\n",
        "            if index_tag != '':\n",
        "                tag_list.append(whole_tag + ',' + str(i - 1))\n",
        "            whole_tag = current_label.replace(begin_label, \"\", 1) + '[' + str(i)\n",
        "            index_tag = current_label.replace(begin_label, \"\", 1)\n",
        "\n",
        "        elif single_label in current_label:\n",
        "            if index_tag != '':\n",
        "                tag_list.append(whole_tag + ',' + str(i - 1))\n",
        "            whole_tag = current_label.replace(single_label, \"\", 1) + '[' + str(i)\n",
        "            tag_list.append(whole_tag)\n",
        "            whole_tag = \"\"\n",
        "            index_tag = \"\"\n",
        "        elif end_label in current_label:\n",
        "            if index_tag != '':\n",
        "                tag_list.append(whole_tag + ',' + str(i))\n",
        "            whole_tag = ''\n",
        "            index_tag = ''\n",
        "        else:\n",
        "            continue\n",
        "    if (whole_tag != '') & (index_tag != ''):\n",
        "        tag_list.append(whole_tag)\n",
        "    tag_list_len = len(tag_list)\n",
        "\n",
        "    for i in range(0, tag_list_len):\n",
        "        if len(tag_list[i]) > 0:\n",
        "            tag_list[i] = tag_list[i] + ']'\n",
        "            insert_list = reverse_style(tag_list[i])\n",
        "            stand_matrix.append(insert_list)\n",
        "    # print stand_matrix\n",
        "    return stand_matrix\n",
        "\n",
        "\n",
        "def save_model(path, model, epoch):\n",
        "    pass\n",
        "\n",
        "def load_model(path, model):\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "    pass\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tMoIToQfZsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "def log_sum_exp(vec, m_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        vec: size=(batch_size, vanishing_dim, hidden_dim)\n",
        "        m_size: hidden_dim\n",
        "\n",
        "    Returns:\n",
        "        size=(batch_size, hidden_dim)\n",
        "    \"\"\"\n",
        "    _, idx = torch.max(vec, 1)  # B * 1 * M\n",
        "    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n",
        "    return max_score.view(-1, m_size) + torch.log(torch.sum(\n",
        "        torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)\n",
        "\n",
        "\n",
        "class CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            target_size: int, target size\n",
        "            use_cuda: bool, 鏄惁浣跨敤gpu, default is True\n",
        "            average_batch: bool, loss鏄惁浣滃钩鍧ꣲ default is True\n",
        "        \"\"\"\n",
        "        super(CRF, self).__init__()\n",
        "        for k in kwargs:\n",
        "            self.__setattr__(k, kwargs[k])\n",
        "        self.START_TAG_IDX, self.END_TAG_IDX = -2, -1\n",
        "        init_transitions = torch.zeros(self.target_size+2, self.target_size+2)\n",
        "        init_transitions[:, self.START_TAG_IDX] = -1000.\n",
        "        init_transitions[self.END_TAG_IDX, :] = -1000.\n",
        "        if self.use_cuda:\n",
        "            init_transitions = init_transitions.cuda()\n",
        "        self.transitions = nn.Parameter(init_transitions)\n",
        "    def _forward_alg(self, feats, mask=None):\n",
        "        \"\"\"\n",
        "        Do the forward algorithm to compute the partition function (batched).\n",
        "\n",
        "        Args:\n",
        "            feats: size=(batch_size, seq_len, self.target_size+2)\n",
        "            mask: size=(batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            xxx\n",
        "        \"\"\"\n",
        "        batch_size = feats.size(0)\n",
        "        seq_len = feats.size(1)\n",
        "        tag_size = feats.size(-1)\n",
        "\n",
        "        mask = mask.transpose(1, 0).contiguous()\n",
        "        ins_num = batch_size * seq_len\n",
        "        feats = feats.transpose(1, 0).contiguous().view(\n",
        "            ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n",
        "\n",
        "        scores = feats + self.transitions.view(\n",
        "            1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n",
        "        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n",
        "        seq_iter = enumerate(scores)\n",
        "        try:\n",
        "            _, inivalues = seq_iter.__next__()\n",
        "        except:\n",
        "            _, inivalues = seq_iter.next()\n",
        "\n",
        "        partition = inivalues[:, self.START_TAG_IDX, :].clone().view(batch_size, tag_size, 1)\n",
        "        for idx, cur_values in seq_iter:\n",
        "            cur_values = cur_values + partition.contiguous().view(\n",
        "                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
        "            cur_partition = log_sum_exp(cur_values, tag_size)\n",
        "            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n",
        "            masked_cur_partition = cur_partition.masked_select(mask_idx.byte())\n",
        "            if masked_cur_partition.dim() != 0:\n",
        "                mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n",
        "                partition.masked_scatter_(mask_idx.byte(), masked_cur_partition)\n",
        "        cur_values = self.transitions.view(1, tag_size, tag_size).expand(\n",
        "            batch_size, tag_size, tag_size) + partition.contiguous().view(\n",
        "                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
        "        cur_partition = log_sum_exp(cur_values, tag_size)\n",
        "        final_partition = cur_partition[:, self.END_TAG_IDX]\n",
        "        return final_partition.sum(), scores\n",
        "        \n",
        "    def _viterbi_decode(self, feats, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feats: size=(batch_size, seq_len, self.target_size+2)\n",
        "            mask: size=(batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            decode_idx: (batch_size, seq_len), viterbi decode缁撴灉\n",
        "            path_score: size=(batch_size, 1), 姣忎釜鍙ュ瓙鐨勫緱鍒ꓱ        \"\"\"\n",
        "        batch_size = feats.size(0)\n",
        "        seq_len = feats.size(1)\n",
        "        tag_size = feats.size(-1)\n",
        "\n",
        "        length_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n",
        "        mask = mask.transpose(1, 0).contiguous()\n",
        "        ins_num = seq_len * batch_size\n",
        "        feats = feats.transpose(1, 0).contiguous().view(\n",
        "            ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n",
        "\n",
        "        scores = feats + self.transitions.view(\n",
        "            1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n",
        "        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n",
        "\n",
        "        seq_iter = enumerate(scores)\n",
        "        # record the position of the best score\n",
        "        back_points = list()\n",
        "        partition_history = list()\n",
        "        mask = (1 - mask.long()).byte()\n",
        "        try:\n",
        "            _, inivalues = seq_iter.__next__()\n",
        "        except:\n",
        "            _, inivalues = seq_iter.next()\n",
        "        partition = inivalues[:, self.START_TAG_IDX, :].clone().view(batch_size, tag_size, 1)\n",
        "        partition_history.append(partition)\n",
        "\n",
        "        for idx, cur_values in seq_iter:\n",
        "            cur_values = cur_values + partition.contiguous().view(\n",
        "                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
        "            partition, cur_bp = torch.max(cur_values, 1)\n",
        "            partition_history.append(partition.unsqueeze(-1))\n",
        "\n",
        "            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n",
        "            back_points.append(cur_bp)\n",
        "\n",
        "        partition_history = torch.cat(partition_history).view(\n",
        "            seq_len, batch_size, -1).transpose(1, 0).contiguous()\n",
        "\n",
        "        last_position = length_mask.view(batch_size, 1, 1).expand(batch_size, 1, tag_size) - 1\n",
        "        last_partition = torch.gather(\n",
        "            partition_history, 1, last_position).view(batch_size, tag_size, 1)\n",
        "\n",
        "        last_values = last_partition.expand(batch_size, tag_size, tag_size) + \\\n",
        "            self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n",
        "        _, last_bp = torch.max(last_values, 1)\n",
        "        pad_zero = Variable(torch.zeros(batch_size, tag_size)).long()\n",
        "        if self.use_cuda:\n",
        "            pad_zero = pad_zero.cuda()\n",
        "        back_points.append(pad_zero)\n",
        "        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n",
        "\n",
        "        pointer = last_bp[:, self.END_TAG_IDX]\n",
        "        insert_last = pointer.contiguous().view(batch_size, 1, 1).expand(batch_size, 1, tag_size)\n",
        "        back_points = back_points.transpose(1, 0).contiguous()\n",
        "\n",
        "        back_points.scatter_(1, last_position, insert_last)\n",
        "\n",
        "        back_points = back_points.transpose(1, 0).contiguous()\n",
        "\n",
        "        decode_idx = Variable(torch.LongTensor(seq_len, batch_size))\n",
        "        if self.use_cuda:\n",
        "            decode_idx = decode_idx.cuda()\n",
        "        decode_idx[-1] = pointer.data\n",
        "        for idx in range(len(back_points)-2, -1, -1):\n",
        "            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n",
        "            decode_idx[idx] = pointer.view(-1).data\n",
        "        path_score = None\n",
        "        decode_idx = decode_idx.transpose(1, 0)\n",
        "        return path_score, decode_idx\n",
        "\n",
        "    def forward(self, feats, mask=None):\n",
        "        path_score, best_path = self._viterbi_decode(feats, mask)\n",
        "        return path_score, best_path\n",
        "\n",
        "    def _score_sentence(self, scores, mask, tags):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            scores: size=(seq_len, batch_size, tag_size, tag_size)\n",
        "            mask: size=(batch_size, seq_len)\n",
        "            tags: size=(batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            score:\n",
        "        \"\"\"\n",
        "        batch_size = scores.size(1)\n",
        "        seq_len = scores.size(0)\n",
        "        tag_size = scores.size(-1)\n",
        "\n",
        "        new_tags = Variable(torch.LongTensor(batch_size, seq_len))\n",
        "        if self.use_cuda:\n",
        "            new_tags = new_tags.cuda()\n",
        "        for idx in range(seq_len):\n",
        "            if idx == 0:\n",
        "                new_tags[:, 0] = (tag_size - 2) * tag_size + tags[:, 0]\n",
        "            else:\n",
        "                new_tags[:, idx] = tags[:, idx-1] * tag_size + tags[:, idx]\n",
        "\n",
        "        end_transition = self.transitions[:, self.END_TAG_IDX].contiguous().view(\n",
        "            1, tag_size).expand(batch_size, tag_size)\n",
        "        length_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n",
        "        end_ids = torch.gather(tags, 1, length_mask-1)\n",
        "\n",
        "        end_energy = torch.gather(end_transition, 1, end_ids)\n",
        "\n",
        "        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n",
        "        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(\n",
        "            seq_len, batch_size)\n",
        "        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n",
        "\n",
        "        gold_score = tg_energy.sum() + end_energy.sum()\n",
        "\n",
        "        return gold_score\n",
        "\n",
        "    def neg_log_likelihood_loss(self, feats, mask, tags):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feats: size=(batch_size, seq_len, tag_size)\n",
        "            mask: size=(batch_size, seq_len)\n",
        "            tags: size=(batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size = feats.size(0)\n",
        "        mask = mask.byte()\n",
        "        forward_score, scores = self._forward_alg(feats, mask)\n",
        "        gold_score = self._score_sentence(scores, mask, tags)\n",
        "        if self.average_batch:\n",
        "            return (forward_score - gold_score) / batch_size\n",
        "        return forward_score - gold_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AVqQ2PUhWId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "e44a54ba-78a7-40cd-c0f6-a6b5a22a3c44"
      },
      "source": [
        "! pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.13.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.16.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.4->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46kR8MiVgBU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "class BERT_LSTM_CRF(nn.Module):\n",
        "    def __init__(self, bert_config, tagset_size, embedding_dim, hidden_dim, rnn_layers, dropout_ratio, dropout1, use_cuda):\n",
        "        super(BERT_LSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeds = BertModel.from_pretrained(bert_config)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                            num_layers=rnn_layers, bidirectional=True, dropout=dropout_ratio, batch_first=True)\n",
        "        self.rnn_layers = rnn_layers\n",
        "        self.dropout1 = nn.Dropout(p=dropout1)\n",
        "        self.crf = CRF(target_size=tagset_size, average_batch=True, use_cuda=use_cuda)\n",
        "        self.liner = nn.Linear(hidden_dim*2, tagset_size+2)\n",
        "        self.tagset_size = tagset_size\n",
        "        self.use_cuda =  use_cuda\n",
        "    def rand_init_hidden(self, batch_size):\n",
        "        if self.use_cuda:\n",
        "            return Variable(\n",
        "                torch.randn(2 * self.rnn_layers, batch_size, self.hidden_dim)).cuda(), Variable(\n",
        "                torch.randn(2 * self.rnn_layers, batch_size, self.hidden_dim)).cuda()\n",
        "        else:\n",
        "            return Variable(\n",
        "                torch.randn(2 * self.rnn_layers, batch_size, self.hidden_dim)), Variable(\n",
        "                torch.randn(2 * self.rnn_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "    def get_output_score(self, sentence, attention_mask=None):\n",
        "        batch_size = sentence.size(0)\n",
        "        seq_length = sentence.size(1)\n",
        "        embeds, _ = self.word_embeds(sentence, attention_mask=attention_mask, output_all_encoded_layers=False)\n",
        "        hidden = self.rand_init_hidden(batch_size)\n",
        "        # if embeds.is_cuda:\n",
        "        #     hidden = (i.cuda() for i in hidden)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim * 2)\n",
        "        d_lstm_out = self.dropout1(lstm_out)\n",
        "        l_out = self.liner(d_lstm_out)\n",
        "        lstm_feats = l_out.contiguous().view(batch_size, seq_length, -1)\n",
        "        return lstm_feats\n",
        "\n",
        "    def forward(self, sentence, masks):\n",
        "        lstm_feats = self.get_output_score(sentence)\n",
        "        scores, tag_seq = self.crf._viterbi_decode(lstm_feats, masks.byte())\n",
        "        return tag_seq\n",
        "\n",
        "    def neg_log_likelihood_loss(self, sentence, mask, tags):\n",
        "        lstm_feats = self.get_output_score(sentence)\n",
        "        loss_value = self.crf.neg_log_likelihood_loss(lstm_feats, mask, tags)\n",
        "        batch_size = lstm_feats.size(0)\n",
        "        loss_value /= float(batch_size)\n",
        "        return loss_value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl2Wc_qZgVKZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "2186c583-cb52-4e37-8a86-ac35e46d82bb"
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\", 0)\n",
        "    print('device',device)\n",
        "    use_cuda = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "vocab = load_vocab(vocab_file)\n",
        "####璇诲彇璁粌闆?\n",
        "print('max_length',max_length)\n",
        "train_data = load_data(train_file, max_length=max_length, label_dic=l2i_dic, vocab=vocab)\n",
        "train_ids = torch.LongTensor([temp.input_id for temp in train_data])\n",
        "train_masks = torch.LongTensor([temp.input_mask for temp in train_data])\n",
        "train_tags = torch.LongTensor([temp.label_id for temp in train_data])\n",
        "train_dataset = TensorDataset(train_ids, train_masks, train_tags)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "#######璇诲彇娴嬭瘯闆?\n",
        "dev_data = load_data(dev_file, max_length=max_length, label_dic=l2i_dic, vocab=vocab)\n",
        "dev_ids = torch.LongTensor([temp.input_id for temp in dev_data])\n",
        "dev_masks = torch.LongTensor([temp.input_mask for temp in dev_data])\n",
        "dev_tags = torch.LongTensor([temp.label_id for temp in dev_data])\n",
        "dev_dataset = TensorDataset(dev_ids, dev_masks, dev_tags)\n",
        "dev_loader = DataLoader(dev_dataset, shuffle=True, batch_size=batch_size)\n",
        "######娴嬭瘯鍑芥暟\n",
        "def evaluate(medel, dev_loader):\n",
        "    medel.eval()\n",
        "    pred = []\n",
        "    gold = []\n",
        "    print('evaluate')\n",
        "    for i, dev_batch in enumerate(dev_loader):\n",
        "        sentence, masks, tags = dev_batch\n",
        "        sentence, masks, tags = Variable(sentence), Variable(masks), Variable(tags)\n",
        "        if use_cuda:\n",
        "            sentence = sentence.cuda()\n",
        "            masks = masks.cuda()\n",
        "            tags = tags.cuda()\n",
        "        predict_tags = medel(sentence, masks)\n",
        "        pred.extend([t for t in predict_tags.tolist()])\n",
        "        gold.extend([t for t in tags.tolist()])\n",
        "    pred_label,gold_label = recover_label(pred, gold, l2i_dic,i2l_dic)\n",
        "    acc, p, r, f = get_ner_fmeasure(gold_label,pred_label)\n",
        "    print(get_right_tag(gold_label,pred_label))\n",
        "    print('p: {}锛宺: {}, f: {}'.format(p, r, f))\n",
        "    model.train()\n",
        "    return acc, p, r, f\n",
        "########鍔犺浇妯″瀷\n",
        "model = BERT_LSTM_CRF('/content/drive/My Drive/chinese_roberta_wwm_ext_pytorch.zip_files', tagset_size, 768, 250, 1,\n",
        "                      dropout_ratio=0.4, dropout1=0.4, use_cuda = use_cuda)\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "model.train()\n",
        "optimizer = getattr(optim, 'Adam')\n",
        "optimizer = optimizer(model.parameters(), lr=0.00003, weight_decay=0.00005)\n",
        "best_f = -100\n",
        "for epoch in range(epochs):\n",
        "    print('epoch: {}锛宼rain'.format(epoch))\n",
        "    for i, train_batch in enumerate(tqdm(train_loader)):\n",
        "        model.zero_grad()\n",
        "        sentence, masks, tags = train_batch\n",
        "        sentence, masks, tags = Variable(sentence), Variable(masks), Variable(tags)\n",
        "        if use_cuda:\n",
        "            sentence = sentence.cuda()\n",
        "            masks = masks.cuda()\n",
        "            tags = tags.cuda()\n",
        "        loss = model.neg_log_likelihood_loss(sentence, masks, tags)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('epoch: {}锛宭oss: {}'.format(epoch, loss.item()))\n",
        "    acc, p, r, f = evaluate(model,dev_loader)\n",
        "    if f > best_f:\n",
        "        model_name = save_model_dir + '.' + str(epochs) + \".pkl\"\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "        best_f = f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device cuda:0\n",
            "max_length 250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1368 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0锛宼rain\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1368/1368 [19:35<00:00,  1.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0锛宭oss: 69.65026092529297\n",
            "evaluate\n",
            "gold_num =  1421  pred_num =  0  right_num =  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8d257330c1e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: {}锛宭oss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8d257330c1e9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(medel, dev_loader)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgold_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecover_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2i_dic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi2l_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ner_fmeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_right_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p: {}锛宺: {}, f: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ca68f14dde7b>\u001b[0m in \u001b[0;36mget_right_tag\u001b[0;34m(golden_lists, predict_lists, label_type)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mf_measure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mright_tag\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgolden_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1969\u001b[0m     \"\"\"\n\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1421, 0]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKK1aOxSLpgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_loader.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpdm4m0vhn7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d81d7c9-0048-4f25-aa2d-6a02f1d38fe1"
      },
      "source": [
        "import torch\n",
        "model =BERT_LSTM_CRF('/content/drive/My Drive/chinese_roberta_wwm_ext_pytorch.zip_files', tagset_size, 768, 200, 1,\n",
        "                      dropout_ratio=0.4, dropout1=0.4, use_cuda = use_cuda)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/bert-bilstm-crf-master/bert_lstm_crf_pytorch/model.100.pkl'))\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT_LSTM_CRF(\n",
              "  (word_embeds): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 200, batch_first=True, dropout=0.4, bidirectional=True)\n",
              "  (dropout1): Dropout(p=0.4, inplace=False)\n",
              "  (crf): CRF()\n",
              "  (liner): Linear(in_features=400, out_features=54, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9ujbpJkOgQI",
        "colab_type": "text"
      },
      "source": [
        "预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MabzI7D7zn4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=open(\"/content/drive/My Drive/Colab Notebooks/kashgari/kashgari/data0/data_unlabeled/abstract_012.txt\",\"r\")\n",
        "for line in f:\n",
        "    line=f.read()\n",
        "    line=line.replace(\"\\n\",\"\")\n",
        "    raw=line.split(\"。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frr54J1q4otD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pre_data=[]\n",
        "for i in raw:\n",
        "  sen=[]\n",
        "  for j in i:\n",
        "    sen.append(j)\n",
        "  pre_data.append(sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPfeMC_1A2NB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "vocab = load_vocab(vocab_file)\n",
        "pre_ids = []\n",
        "pre_masks=[]\n",
        "for i in range(len(pre_data)):\n",
        "    token = pre_data[i]\n",
        "    if len(token) > max_length-2:\n",
        "        token = token[0:(max_length-2)]\n",
        "    tokens_f =['[CLS]'] + token + ['[SEP]']\n",
        "    input_ids = [int(vocab[i]) if i in vocab else int(vocab['[UNK]']) for i in tokens_f]\n",
        "    mask_bool=1\n",
        "    input_mask = [mask_bool] * len(input_ids)\n",
        "    while len(input_ids) < max_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "    assert len(input_ids) == max_length\n",
        "    assert len(input_mask) == max_length\n",
        "    pre_ids.append(input_ids)\n",
        "    pre_masks.append(input_mask)\n",
        "pre_ids_f=torch.LongTensor(pre_ids)\n",
        "pre_masks_f=torch.LongTensor(pre_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkR-jEzfkIwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "pre_dataset=TensorDataset(pre_ids_f, pre_masks_f)\n",
        "pre_loader = DataLoader(pre_dataset, shuffle=False, batch_size=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCyM_VnAI6tO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pred=[]\n",
        "from torch.autograd import Variable\n",
        "model.eval()\n",
        "entities_list=[]\n",
        "type_list=[]\n",
        "for i, pre_batch in enumerate(pre_loader):\n",
        "    sentence, masks = pre_batch\n",
        "    sentence, masks = Variable(sentence), Variable(masks)\n",
        "    if use_cuda:\n",
        "        sentence = sentence.cuda()\n",
        "        masks = masks.cuda()\n",
        "    predict_tags = model(sentence, masks)\n",
        "    for i in range(len(predict_tags)):\n",
        "        tp=[]\n",
        "        types=[]\n",
        "        entities=[]\n",
        "        entity=[]\n",
        "        for j in range(len(predict_tags[i])):\n",
        "            if (predict_tags[i][j] !=0 and predict_tags[i][j] !=50 and predict_tags[i][j] !=51) and (predict_tags[i][j]>=predict_tags[i][j-1]):\n",
        "                entity.append(j)\n",
        "                tp.append(i2l_dic[predict_tags.tolist()[i][j]])\n",
        "            else:\n",
        "                if entity !=[]:\n",
        "                    entities.append(entity)\n",
        "                    types.append(tp)\n",
        "                entity=[]\n",
        "                tp=[]\n",
        "        entities_list.append(entities)\n",
        "        type_list.append(types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xRTcW2eVtMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recovered_entities=[]\n",
        "for i in  range(len(entities_list)):\n",
        "    recovered_word=[]\n",
        "    for j in range(len(entities_list[i])):\n",
        "        recover_char=\"\"\n",
        "        for m in range(len(entities_list[i][j])):\n",
        "            try:\n",
        "                recover_char+=pre_data[i][entities_list[i][j][m]-1]\n",
        "            except:\n",
        "                pass\n",
        "        recovered_word.append(recover_char)\n",
        "    recovered_entities.append(recovered_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koIXz2pUIzgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities_type=[]\n",
        "for i in range(len(recovered_entities)):\n",
        "    entity_type=[]\n",
        "    for j in range(len(recovered_entities[i])):\n",
        "        enti_dic={}\n",
        "        enti_dic={type_list[i][j][0][2:]:recovered_entities[i][j]}\n",
        "        entity_type.append(enti_dic)\n",
        "    entities_type.append(entity_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuX6FM2fLUGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities_sen=list(zip(raw,entities_type))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNqN284pkn1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rel_sen=[]\n",
        "for i in entities_sen:\n",
        "    for j in i[1]:\n",
        "        for k in i[1]:\n",
        "            if i[1].index(j)<=i[1].index(k):\n",
        "                rel_sen.append((list(j.keys())[0],list(j.values())[0],list(k.keys())[0],list(k.values())[0],i[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5S4lWldCCLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(rel_sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgFCxwlvE92m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"/content/drive/My Drive/Colab Notebooks/bert-bilstm-crf-master/bert_lstm_crf_pytorch/data/12.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qUK2iJPO8Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import warnings\n",
        "from torch.utils.data import TensorDataset\n",
        "warnings.filterwarnings('ignore')\n",
        "str_file_nums=[]\n",
        "file_nums=range(12,153)\n",
        "for i in file_nums:\n",
        "  if i<100:\n",
        "    str_file_nums.append(\"0\"+str(i))\n",
        "  else:\n",
        "    str_file_nums.append(str(i))\n",
        "for num in range(len(str_file_nums)):\n",
        "  file=\"/content/drive/My Drive/Colab Notebooks/kashgari/kashgari/data0/data_unlabeled/abstract_\"+str_file_nums[num]+\".txt\"\n",
        "  with open(file,encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        line=f.read()\n",
        "        line=line.replace(\"\\n\",\"\")\n",
        "        raw=line.split(\"。\")\n",
        "    pre_data=[]\n",
        "    for i in raw:\n",
        "      sen=[]\n",
        "      for j in i:\n",
        "        sen.append(j)\n",
        "      pre_data.append(sen)\n",
        "    vocab = load_vocab(vocab_file)\n",
        "    pre_ids = []\n",
        "    pre_masks=[]\n",
        "    for i in range(len(pre_data)):\n",
        "        token = pre_data[i]\n",
        "        if len(token) > max_length-2:\n",
        "            token = token[0:(max_length-2)]\n",
        "        tokens_f =['[CLS]'] + token + ['[SEP]']\n",
        "        input_ids = [int(vocab[i]) if i in vocab else int(vocab['[UNK]']) for i in tokens_f]\n",
        "        mask_bool=1\n",
        "        input_mask = [mask_bool] * len(input_ids)\n",
        "        while len(input_ids) < max_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "        assert len(input_ids) == max_length\n",
        "        assert len(input_mask) == max_length\n",
        "        pre_ids.append(input_ids)\n",
        "        pre_masks.append(input_mask)\n",
        "    pre_ids_f=torch.LongTensor(pre_ids)\n",
        "    pre_masks_f=torch.LongTensor(pre_masks)\n",
        "    pre_dataset=TensorDataset(pre_ids_f, pre_masks_f)\n",
        "    pre_loader = DataLoader(pre_dataset, shuffle=False, batch_size=12)\n",
        "    pred=[]\n",
        "    from torch.autograd import Variable\n",
        "    model.eval()\n",
        "    entities_list=[]\n",
        "    type_list=[]\n",
        "    for i, pre_batch in enumerate(pre_loader):\n",
        "        sentence, masks = pre_batch\n",
        "        sentence, masks = Variable(sentence), Variable(masks)\n",
        "        if use_cuda:\n",
        "            sentence = sentence.cuda()\n",
        "            masks = masks.cuda()\n",
        "        predict_tags = model(sentence, masks)\n",
        "        for i in range(len(predict_tags)):\n",
        "            tp=[]\n",
        "            types=[]\n",
        "            entities=[]\n",
        "            entity=[]\n",
        "            for j in range(len(predict_tags[i])):\n",
        "                if (predict_tags[i][j] !=0 and predict_tags[i][j] !=50 and predict_tags[i][j] !=51) and (predict_tags[i][j]>=predict_tags[i][j-1]):\n",
        "                    entity.append(j)\n",
        "                    tp.append(i2l_dic[predict_tags.tolist()[i][j]])\n",
        "                else:\n",
        "                    if entity !=[]:\n",
        "                        entities.append(entity)\n",
        "                        types.append(tp)\n",
        "                    entity=[]\n",
        "                    tp=[]\n",
        "            entities_list.append(entities)\n",
        "            type_list.append(types)\n",
        "    recovered_entities=[]\n",
        "    for i in  range(len(entities_list)):\n",
        "        recovered_word=[]\n",
        "        for j in range(len(entities_list[i])):\n",
        "            recover_char=\"\"\n",
        "            for m in range(len(entities_list[i][j])):\n",
        "                try:\n",
        "                    recover_char+=pre_data[i][entities_list[i][j][m]-1]\n",
        "                except:\n",
        "                    pass\n",
        "            recovered_word.append(recover_char)\n",
        "        recovered_entities.append(recovered_word)\n",
        "    entities_type=[]\n",
        "    for i in range(len(recovered_entities)):\n",
        "        entity_type=[]\n",
        "        for j in range(len(recovered_entities[i])):\n",
        "            enti_dic={}\n",
        "            enti_dic={type_list[i][j][0][2:]:recovered_entities[i][j]}\n",
        "            entity_type.append(enti_dic)\n",
        "        entities_type.append(entity_type)\n",
        "    entities_sen=list(zip(raw,entities_type))\n",
        "    rel_sen=[]\n",
        "    for i in entities_sen:\n",
        "        for j in i[1]:\n",
        "            for k in i[1]:\n",
        "                if i[1].index(j)<i[1].index(k):\n",
        "                    rel_sen.append((list(j.keys())[0],list(j.values())[0],list(k.keys())[0],list(k.values())[0],i[0]))\n",
        "    df=pd.DataFrame(rel_sen)\n",
        "    df.to_csv(\"/content/drive/My Drive/Colab Notebooks/bert-bilstm-crf-master/bert_lstm_crf_pytorch/data/\"+str_file_nums[num]+\".txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7sRtY1CM3j5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def get_right_tag(golden_lists, predict_lists, label_type=\"BMES\"):\n",
        "    sent_num = len(golden_lists)\n",
        "    golden_full = []\n",
        "    predict_full = []\n",
        "    right_full = []\n",
        "    right_tag = 0\n",
        "    all_tag = 0\n",
        "    for idx in range(0,sent_num):\n",
        "        # word_list = sentence_lists[idx]\n",
        "        golden_list = golden_lists[idx]\n",
        "        predict_list = predict_lists[idx]\n",
        "        for idy in range(len(golden_list)):\n",
        "            if golden_list[idy] == predict_list[idy]:\n",
        "                right_tag += 1\n",
        "        all_tag += len(golden_list)\n",
        "        if label_type == \"BMES\":\n",
        "            gold_matrix = get_ner_BMES(golden_list)\n",
        "            pred_matrix = get_ner_BMES(predict_list)\n",
        "        else:\n",
        "            gold_matrix = get_ner_BIO(golden_list)\n",
        "            pred_matrix = get_ner_BIO(predict_list)\n",
        "        right_ner = list(set(gold_matrix).intersection(set(pred_matrix)))\n",
        "        golden_full += gold_matrix\n",
        "        predict_full += pred_matrix\n",
        "        right_full += right_ner\n",
        "    right_num = len(right_full)\n",
        "    golden_num = len(golden_full)\n",
        "    predict_num = len(predict_full)\n",
        "    if predict_num == 0:\n",
        "        precision = -1\n",
        "    else:\n",
        "        precision =  (right_num+0.0)/predict_num\n",
        "    if golden_num == 0:\n",
        "        recall = -1\n",
        "    else:\n",
        "        recall = (right_num+0.0)/golden_num\n",
        "    if (precision == -1) or (recall == -1) or (precision+recall) <= 0.:\n",
        "        f_measure = -1\n",
        "    else:\n",
        "        f_measure = 2*precision*recall/(precision+recall)\n",
        "    accuracy = (right_tag+0.0)/all_tag\n",
        "    print(classification_report(golden_full,predict_full))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-LMh0ubC3L-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}